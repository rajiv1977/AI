{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1StfPdo_A8gsZKK72NiTruLn498G23pDn",
      "authorship_tag": "ABX9TyNXxAiPArSASxoao/210Esa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajiv1977/AI/blob/main/CNNTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# TEST DEV\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qt7S6Xi8njIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_CEUrdor1hh",
        "outputId": "e107b9f8-99b0-4d33-ed82-5875b1f01389"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "lRUSdz5vilmN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProcessingSettings:\n",
        "    \"\"\"Struct to hold settings for image processing.\"\"\"\n",
        "\n",
        "    def __init__(self, convert_to_gray=False, resize=False, resize_width=1, resize_height=1, rotate=False, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize settings for image processing.\n",
        "\n",
        "        Args:\n",
        "            convert_to_gray (bool): Flag to convert images to grayscale.\n",
        "            resize (bool): Flag to indicate if resizing is needed.\n",
        "            resize_width (int): Downscale factor for width.\n",
        "            resize_height (int): Downscale factor for height.\n",
        "            rotate (bool): Flag to indicate if rotation is needed.\n",
        "            num_classes (int): Number of classes or features.\n",
        "        \"\"\"\n",
        "        self.convert_to_gray = convert_to_gray\n",
        "        self.resize = resize\n",
        "        self.resize_width = resize_width  # New attribute for resize width\n",
        "        self.resize_height = resize_height  # New attribute for resize height\n",
        "        self.rotate = rotate\n",
        "        self.num_classes = num_classes"
      ],
      "metadata": {
        "id": "9aKVzDTIioyb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder, settings):\n",
        "    \"\"\"\n",
        "    Load images from specified folder and its subfolders.\n",
        "\n",
        "    Args:\n",
        "        folder (str): Path to the main dataset folder.\n",
        "        settings (ImageProcessingSettings): Settings for image processing.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Array of loaded images.\n",
        "        np.array: Array of labels corresponding to the images.\n",
        "        list: List of label names.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_names = []\n",
        "\n",
        "    # Get subfolder names (class labels)\n",
        "    subfolders = [f for f in os.listdir(folder) if os.path.isdir(os.path.join(folder, f))]\n",
        "\n",
        "    # Check if any subfolders were found\n",
        "    if not subfolders:\n",
        "        print(\"Error: No subfolders found in the dataset path.\")\n",
        "        return np.array([]), np.array([]), []\n",
        "\n",
        "    label_names = subfolders\n",
        "    num_classes = len(label_names)\n",
        "\n",
        "    # Iterate through subfolders and load images\n",
        "    for label_index, subfolder in enumerate(subfolders):\n",
        "        subfolder_path = os.path.join(folder, subfolder)\n",
        "        for filename in os.listdir(subfolder_path):\n",
        "            img_path = os.path.join(subfolder_path, filename)\n",
        "            try:\n",
        "                # Load image with appropriate color mode\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE if settings.convert_to_gray else cv2.IMREAD_COLOR)\n",
        "\n",
        "                # Check if image loading was successful\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Could not load image: {img_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Resize image if specified\n",
        "                if settings.resize:\n",
        "                    img = cv2.resize(img, (settings.resize_width, settings.resize_height))\n",
        "\n",
        "                # Add channel dimension for grayscale images if needed\n",
        "                if settings.convert_to_gray and len(img.shape) == 2:\n",
        "                    img = img.reshape(img.shape[0], img.shape[1], 1)\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(label_index)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels), label_names\n"
      ],
      "metadata": {
        "id": "RzlukasEitFG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Build a Convolutional Neural Network (CNN) model using Keras.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input images.\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled CNN model.\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Input layer: (height, width, channels)\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same')) # reduced kernel size, added padding\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same')) # reduced kernel size, added padding\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten layer\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))  # Use softmax for multi-class classification\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "eZBBYtFYizBX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Parameters\n",
        "    epochs = 25\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.01  # Not used in this example but included for completeness\n",
        "\n",
        "    # Set the dataset path (update this to your dataset path)\n",
        "    dataset_path = \"/content/drive/My Drive/AI_Outputs/TrainAndValidateData\"\n",
        "\n",
        "    # Verify if the dataset path is valid\n",
        "    if not os.path.isdir(dataset_path):\n",
        "        print(f\"Error: Invalid dataset path: {dataset_path}\")\n",
        "        print(\"Please ensure the path is correct and the directory exists.\")\n",
        "        return  # Exit the program if the path is invalid\n",
        "\n",
        "    # Get a list of subfolders and count them\n",
        "    subfolders = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]\n",
        "    num_classes = len(subfolders)\n",
        "    print(f\"Found {num_classes} subfolders in the dataset path.\")\n",
        "\n",
        "    # Create image processing settings\n",
        "    settings = ImageProcessingSettings(\n",
        "        convert_to_gray=True,\n",
        "        resize=True,\n",
        "        resize_width=40,\n",
        "        resize_height=40,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "\n",
        "    # Load images and labels\n",
        "    images, labels, label_names = load_images_from_folder(dataset_path, settings)\n",
        "    labels = to_categorical(labels, settings.num_classes)\n",
        "\n",
        "    print(\"Number of images loaded:\", len(images))\n",
        "    print(\"Shape of first image:\", images[0].shape)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.3, random_state=1)  # Changed test_size to 0.3\n",
        "\n",
        "    # Print the shapes of X_train and y_train\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "    # Get the actual input shape from the loaded images\n",
        "    input_shape = images[0].shape\n",
        "\n",
        "    # Build the model, now passing the actual input shape\n",
        "    model = build_cnn_model(input_shape, settings.num_classes)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "    # ... (your existing code) ...\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Convert predictions and true labels to class indices\n",
        "    predicted_classes = np.argmax(y_pred, axis=1)\n",
        "    true_classes = np.argmax(y_val, axis=1)\n",
        "\n",
        "    # Print predicted and true y values for the first few samples\n",
        "    num_samples_to_print = 10  # Adjust as needed\n",
        "    for i in range(num_samples_to_print):\n",
        "        print(f\"Sample {i + 1}: Predicted Class: {predicted_classes[i]}, True Class: {true_classes[i]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXbxlMAbi2b3",
        "outputId": "e8f5db44-2dda-420f-b46e-da254aad3b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 subfolders in the dataset path.\n",
            "Warning: Could not load image: /content/drive/My Drive/AI_Outputs/TrainAndValidateData/Planes/Plane (43).gif\n",
            "Number of images loaded: 8789\n",
            "Shape of first image: (40, 40, 1)\n",
            "X_train shape: (6152, 40, 40, 1)\n",
            "y_train shape: (6152, 10)\n",
            "Epoch 1/25\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 201ms/step - accuracy: 0.1856 - loss: 16.7422 - val_accuracy: 0.4983 - val_loss: 1.5245\n",
            "Epoch 2/25\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 190ms/step - accuracy: 0.4265 - loss: 1.6414 - val_accuracy: 0.5461 - val_loss: 1.4054\n",
            "Epoch 3/25\n",
            "\u001b[1m 77/193\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 167ms/step - accuracy: 0.5046 - loss: 1.4041"
          ]
        }
      ]
    }
  ]
}